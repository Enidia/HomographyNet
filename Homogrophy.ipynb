{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### this is used to construct the neural network for img registration\n",
    "### the struct model I used is the HomographyNet\n",
    "\n",
    "1 flow the very method in img registration<br>\n",
    "2 use COCO2017 for further training<br>\n",
    "3 ***in case the train test val are based in image set, so as we have to rebuilt the img set for the project***<br>\n",
    "4 there is suggested that use the map to registration<br>\n",
    "5 us the matrix for registration as result and so as the MSE method in matrix as the loss of further training<br>\n",
    "6 as for the testing, use COCO2017 which we haven't used in training set<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model,self).__init__()\n",
    "        self.layer1 = nn.Sequential(nn.Conv2d(in_channels=2,out_channels=64,kernel_size=3,stride=1,padding=1),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.BatchNorm2d(64))\n",
    "        self.layer2 = nn.Sequential(nn.Conv2d(64,64,3,1,1),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.BatchNorm2d(64),\n",
    "                                    nn.MaxPool2d(2))\n",
    "        self.layer3 = nn.Sequential(nn.Conv2d(64,64,3,1,1),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.BatchNorm2d(64))\n",
    "        \n",
    "        self.layer4 = nn.Sequential(nn.Conv2d(64,64,3,1,1),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.BatchNorm2d(64),\n",
    "                                    nn.MaxPool2d(2))\n",
    "        self.layer5 = nn.Sequential(nn.Conv2d(64,128,3,1,1),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.BatchNorm2d(128))\n",
    "        self.layer6 = nn.Sequential(nn.Conv2d(128,128,3,1,1),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.BatchNorm2d(128),\n",
    "                                    nn.MaxPool2d(2))\n",
    "        self.layer7 = nn.Sequential(nn.Conv2d(128,128,3,1,1),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.BatchNorm2d(128))\n",
    "        self.layer8 = nn.Sequential(nn.Conv2d(128,128,3,1,1),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.BatchNorm2d(128))\n",
    "        self.fc1 = nn.Linear(128*16*16,1024,bias=True)\n",
    "        self.fc2 = nn.Linear(1024,8,bias=True)\n",
    "    def forward(self,x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.layer5(out)\n",
    "        out = self.layer6(out)\n",
    "        out = self.layer7(out)\n",
    "        out = self.layer8(out)\n",
    "        out = out.contiguous().view(-1,128*16*16)\n",
    "        # keep the sequence memeory in case not be matched\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "# images = torch.randn(1,2,128,128).cuda()\n",
    "# model = Model().cuda()\n",
    "# summary(model,(2,128,128))\n",
    "# outputs = model.forward(images)\n",
    "# print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for training set set up and test set set up\n",
    "### the method below are for model training and test\n",
    "### a method of the adjust img with H_four_point\n",
    "### return the batch of each img set, the return format is [(img1,img2)(four minus point)]\n",
    "\n",
    "1 **the great ever change mathod in the matrix manufacture**<br>\n",
    "2 use four point randomly to change with matching the maps<br>\n",
    "3 then we return the map as it with a matrix which represent the adjust method<br>\n",
    "4 use COCO2017 to settle the training and testing set<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\EssentialSoftware\\python\\lib\\site-packages\\numpy\\core\\_asarray.py:136: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order, subok=True)\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "import random\n",
    "from numpy.linalg import inv\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def ImagePreProcessing(image,path):\n",
    "    img = cv2.imread(path+\"/%s\"%image,0)\n",
    "    img = cv2.resize(img,(320,240))\n",
    "    #make it small and easy to train\n",
    "    \n",
    "    rho = 32\n",
    "    patch_size = 128\n",
    "    top_point = (32,32)\n",
    "    left_point = (patch_size+32,32)\n",
    "    bottom_point = (patch_size+32,patch_size+32)\n",
    "    right_point = (32,patch_size+32)\n",
    "    test_image = img.copy()\n",
    "    four_points = [top_point,left_point,bottom_point,right_point]\n",
    "    \n",
    "    perturbed_four_points = []\n",
    "    for point in four_points:\n",
    "        perturbed_four_points.append((point[0] + random.randint(-rho,rho), point[1]+random.randint(-rho,rho)))\n",
    "        # move four point to change the map\n",
    "    H = cv2.getPerspectiveTransform(np.float32(four_points),np.float32(perturbed_four_points))\n",
    "    # use the four point to get the matrix\n",
    "    H_inverse = inv(H)\n",
    "    # does it usefu, it may not in that necessary\n",
    "    warped_image = cv2.warpPerspective(img,H_inverse, (320,240))\n",
    "    # use matrix to get the image adjusted\n",
    "    annotated_warp_image = warped_image.copy()\n",
    "    \n",
    "    Ip1 = test_image[top_point[1]:bottom_point[1],top_point[0]:bottom_point[0]]\n",
    "    # which is the source img\n",
    "    Ip2 = warped_image[top_point[1]:bottom_point[1],top_point[0]:bottom_point[0]]\n",
    "    # which is the goal img             \n",
    "    training_image = np.dstack((Ip1, Ip2))\n",
    "    # in dimension we stack two matrix map as one\n",
    "    H_four_points = np.subtract(np.array(perturbed_four_points), np.array(four_points))\n",
    "    # each point in matrix just match and minus each point in another matrix\n",
    "    datum = (training_image, H_four_points)\n",
    "    # make them as a real batch of training, [img1,img2],[minus matrix]\n",
    "    return datum\n",
    "    \n",
    "def savedata(path):\n",
    "    lst = os.listdir(path)\n",
    "    if not os.path.exists(path+\"processed/\"):\n",
    "        os.makedirs(path+\"processed/\")\n",
    "    new_path = path+\"processed/\"\n",
    "    for i in lst:\n",
    "        np.save(new_path+\"%s\"%i[0:12],ImagePreProcessing(i,path))\n",
    "        # in order to remove jpg, then use i[0:12]\n",
    "savedata(\"./test\")\n",
    "savedata(\"./train\")\n",
    "savedata(\"./val\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the method to get img from processed img Dataset\n",
    "### List are below\n",
    "1 ./trainprocessed folder for the training dataset<br>\n",
    "2 ./testprocessed folder for the testing dataset<br>\n",
    "3 ./valprocessed folder for the validation dataset<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "class Cocodataset():\n",
    "    def __init__(self,path):\n",
    "        X = ()\n",
    "        Y = ()\n",
    "        lst = os.listdir(path)\n",
    "        it = 0\n",
    "        for i in lst:\n",
    "            array = np.load(path+\"%s\"%i,allow_pickle=True)\n",
    "            x = torch.from_numpy((array[0].astype(float)-127.5)/127.5)\n",
    "            # transfer the numpy matrix to tensor and both of them share the same memory\n",
    "            X = X+(x,)\n",
    "            y = torch.from_numpy(array[1].astype(float)/32.)\n",
    "            Y = Y+(y,)\n",
    "            it += 1\n",
    "        self.len = it\n",
    "        self.X_data = X\n",
    "        self.Y_data = Y\n",
    "    def __getitem__(self,index):\n",
    "        return self.X_data[index],self.Y_data[index]\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the training method are below\n",
    "### use MSE as the Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "the Training Dataset are: 1787\n",
      "the Validation Dataset are: 68\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn as func\n",
    "import torch\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "train_path = \"./trainprocessed/\"\n",
    "validation_path = \"./valprocessed/\"\n",
    "\n",
    "TrainingData = Cocodataset(train_path)\n",
    "ValidationData = Cocodataset(validation_path)\n",
    "\n",
    "print(\"the Training Dataset are:\",TrainingData.len)\n",
    "print(\"the Validation Dataset are:\",ValidationData.len)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Then we go for training the Dataset\n",
    "#### iteration and training then get the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 128, 128]           1,216\n",
      "              ReLU-2         [-1, 64, 128, 128]               0\n",
      "       BatchNorm2d-3         [-1, 64, 128, 128]             128\n",
      "            Conv2d-4         [-1, 64, 128, 128]          36,928\n",
      "              ReLU-5         [-1, 64, 128, 128]               0\n",
      "       BatchNorm2d-6         [-1, 64, 128, 128]             128\n",
      "         MaxPool2d-7           [-1, 64, 64, 64]               0\n",
      "            Conv2d-8           [-1, 64, 64, 64]          36,928\n",
      "              ReLU-9           [-1, 64, 64, 64]               0\n",
      "      BatchNorm2d-10           [-1, 64, 64, 64]             128\n",
      "           Conv2d-11           [-1, 64, 64, 64]          36,928\n",
      "             ReLU-12           [-1, 64, 64, 64]               0\n",
      "      BatchNorm2d-13           [-1, 64, 64, 64]             128\n",
      "        MaxPool2d-14           [-1, 64, 32, 32]               0\n",
      "           Conv2d-15          [-1, 128, 32, 32]          73,856\n",
      "             ReLU-16          [-1, 128, 32, 32]               0\n",
      "      BatchNorm2d-17          [-1, 128, 32, 32]             256\n",
      "           Conv2d-18          [-1, 128, 32, 32]         147,584\n",
      "             ReLU-19          [-1, 128, 32, 32]               0\n",
      "      BatchNorm2d-20          [-1, 128, 32, 32]             256\n",
      "        MaxPool2d-21          [-1, 128, 16, 16]               0\n",
      "           Conv2d-22          [-1, 128, 16, 16]         147,584\n",
      "             ReLU-23          [-1, 128, 16, 16]               0\n",
      "      BatchNorm2d-24          [-1, 128, 16, 16]             256\n",
      "           Conv2d-25          [-1, 128, 16, 16]         147,584\n",
      "             ReLU-26          [-1, 128, 16, 16]               0\n",
      "      BatchNorm2d-27          [-1, 128, 16, 16]             256\n",
      "           Linear-28                 [-1, 1024]      33,555,456\n",
      "           Linear-29                    [-1, 8]           8,200\n",
      "================================================================\n",
      "Total params: 34,193,800\n",
      "Trainable params: 34,193,800\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.12\n",
      "Forward/backward pass size (MB): 70.26\n",
      "Params size (MB): 130.44\n",
      "Estimated Total Size (MB): 200.82\n",
      "----------------------------------------------------------------\n",
      "Train Epoch: [1/44] [111/112 (99%)]\\Mean Squared Error: 0.408488\n",
      "Train Epoch: [2/44] [111/112 (99%)]\\Mean Squared Error: 0.318990\n",
      "Train Epoch: [3/44] [111/112 (99%)]\\Mean Squared Error: 0.265613\n",
      "Train Epoch: [4/44] [111/112 (99%)]\\Mean Squared Error: 0.208116\n",
      "Train Epoch: [5/44] [111/112 (99%)]\\Mean Squared Error: 0.128565\n",
      "Train Epoch: [6/44] [111/112 (99%)]\\Mean Squared Error: 0.166308\n",
      "Train Epoch: [7/44] [111/112 (99%)]\\Mean Squared Error: 0.196945\n",
      "Train Epoch: [8/44] [111/112 (99%)]\\Mean Squared Error: 0.156308\n",
      "Train Epoch: [9/44] [111/112 (99%)]\\Mean Squared Error: 0.094160\n",
      "Train Epoch: [10/44] [111/112 (99%)]\\Mean Squared Error: 0.083078\n",
      "Train Epoch: [11/44] [111/112 (99%)]\\Mean Squared Error: 0.086832\n",
      "Train Epoch: [12/44] [111/112 (99%)]\\Mean Squared Error: 0.060588\n",
      "Train Epoch: [13/44] [111/112 (99%)]\\Mean Squared Error: 0.042719\n",
      "Train Epoch: [14/44] [111/112 (99%)]\\Mean Squared Error: 0.048044\n",
      "Train Epoch: [15/44] [111/112 (99%)]\\Mean Squared Error: 0.041332\n",
      "Train Epoch: [16/44] [111/112 (99%)]\\Mean Squared Error: 0.035627\n",
      "Train Epoch: [17/44] [111/112 (99%)]\\Mean Squared Error: 0.028485\n",
      "Train Epoch: [18/44] [111/112 (99%)]\\Mean Squared Error: 0.029875\n",
      "Train Epoch: [19/44] [111/112 (99%)]\\Mean Squared Error: 0.029604\n",
      "Train Epoch: [20/44] [111/112 (99%)]\\Mean Squared Error: 0.031472\n",
      "Train Epoch: [21/44] [111/112 (99%)]\\Mean Squared Error: 0.018442\n",
      "Train Epoch: [22/44] [111/112 (99%)]\\Mean Squared Error: 0.019137\n",
      "Train Epoch: [23/44] [111/112 (99%)]\\Mean Squared Error: 0.025797\n",
      "Train Epoch: [24/44] [111/112 (99%)]\\Mean Squared Error: 0.029590\n",
      "Train Epoch: [25/44] [111/112 (99%)]\\Mean Squared Error: 0.019877\n",
      "Train Epoch: [26/44] [111/112 (99%)]\\Mean Squared Error: 0.022087\n",
      "Train Epoch: [27/44] [111/112 (99%)]\\Mean Squared Error: 0.017136\n",
      "Train Epoch: [28/44] [111/112 (99%)]\\Mean Squared Error: 0.027849\n",
      "Train Epoch: [29/44] [111/112 (99%)]\\Mean Squared Error: 0.017508\n",
      "Train Epoch: [30/44] [111/112 (99%)]\\Mean Squared Error: 0.017499\n",
      "Train Epoch: [31/44] [111/112 (99%)]\\Mean Squared Error: 0.016987\n",
      "Train Epoch: [32/44] [111/112 (99%)]\\Mean Squared Error: 0.018387\n",
      "Train Epoch: [33/44] [111/112 (99%)]\\Mean Squared Error: 0.017503\n",
      "Train Epoch: [34/44] [111/112 (99%)]\\Mean Squared Error: 0.016871\n",
      "Train Epoch: [35/44] [111/112 (99%)]\\Mean Squared Error: 0.018534\n",
      "Train Epoch: [36/44] [111/112 (99%)]\\Mean Squared Error: 0.011920\n",
      "Train Epoch: [37/44] [111/112 (99%)]\\Mean Squared Error: 0.015182\n",
      "Train Epoch: [38/44] [111/112 (99%)]\\Mean Squared Error: 0.016672\n",
      "Train Epoch: [39/44] [111/112 (99%)]\\Mean Squared Error: 0.021760\n",
      "Train Epoch: [40/44] [111/112 (99%)]\\Mean Squared Error: 0.026965\n",
      "Train Epoch: [41/44] [111/112 (99%)]\\Mean Squared Error: 0.016478\n",
      "Train Epoch: [42/44] [111/112 (99%)]\\Mean Squared Error: 0.013923\n",
      "Train Epoch: [43/44] [111/112 (99%)]\\Mean Squared Error: 0.014236\n",
      "Train Epoch: [44/44] [111/112 (99%)]\\Mean Squared Error: 0.013130\n",
      "model saved!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchsummary import summary\n",
    "torch.cuda.empty_cache()\n",
    "batch_size = 16\n",
    "TrainLoader = DataLoader(TrainingData,batch_size)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "num_samples = TrainingData.len\n",
    "total_iteration = 5000\n",
    "steps_per_epoch = num_samples / batch_size\n",
    "\n",
    "epochs = int(total_iteration / steps_per_epoch)\n",
    "\n",
    "model = Model().to(device)\n",
    "summary(model,(2,128,128))\n",
    "optimizer = optim.SGD(model.parameters(),lr=0.005,momentum=0.9)\n",
    "scheduler = lr_scheduler.StepLR(optimizer,step_size=30000,gamma=0.1)\n",
    "for epoch in range(epochs):\n",
    "    for i,(images,target) in enumerate(TrainLoader):\n",
    "        optimizer.zero_grad() \n",
    "        images = images.to(device)\n",
    "        target = target.to(device)\n",
    "        images = images.permute(0,3,1,2).float()\n",
    "        target = target.float()\n",
    "        outputs = model.forward(images)\n",
    "        # print(outputs.size())\n",
    "        # print(target.view(-1,8).size())\n",
    "        loss = criterion(outputs,target.view(-1,8))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        if (i+1) % len(TrainLoader) == 0:\n",
    "            print('Train Epoch: [{}/{}] [{}/{} ({:.0f}%)]\\Mean Squared Error: {:.6f}'.format(\n",
    "                epoch+1,epochs, i , len(TrainLoader),\n",
    "                100. * i / len(TrainLoader), loss))\n",
    "        \n",
    "state = {'epoch': epochs, 'state_dict': model.state_dict(),\n",
    "             'optimizer': optimizer.state_dict() }\n",
    "torch.save(state, './DeepHomographyNet.pth')\n",
    "print(\"model saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for eval the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer5): Sequential(\n",
      "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (layer6): Sequential(\n",
      "    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer7): Sequential(\n",
      "    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (layer8): Sequential(\n",
      "    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (fc1): Linear(in_features=32768, out_features=1024, bias=True)\n",
      "  (fc2): Linear(in_features=1024, out_features=8, bias=True)\n",
      ")\n",
      "tensor(0.3257, device='cuda:0')\n",
      "tensor(0.3256, device='cuda:0')\n",
      "tensor(0.3010, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn\n",
    "from torch.utils.data import DataLoader\n",
    "criterion = nn.MSELoss()\n",
    "batch_size = 32\n",
    "ValidationLoader =DataLoader(ValidationData,batch_size)\n",
    "model = Model().cuda()\n",
    "\n",
    "# criterion = nn.MSELoss()\n",
    "# from torchsummary import summary\n",
    "# summary(model,(2,128,128)) \n",
    "# must formal\n",
    "model.load_state_dict(torch.load('DeepHomographyNet.pth')['state_dict'])\n",
    "# print(model)\n",
    "\n",
    "print(model)\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i,(images,target) in enumerate(ValidationLoader):\n",
    "        images = images.to(device)\n",
    "        target = target.to(device)\n",
    "        images = images.permute(0,3,1,2).float()\n",
    "        target = target.float()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs,target.view(-1,8))\n",
    "        print(loss)\n",
    "    # print(\"\\Mean Squared Error: {:.6f}\".format(np.mean(loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Revise the Image\n",
    "\n",
    "Repair it\n",
    "the image which is aim to fix, by which, we can find its points can be find as a stable one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the TestData set are 17\n",
      "[[[153 140]\n",
      "  [155 139]\n",
      "  [162 139]\n",
      "  ...\n",
      "  [153 133]\n",
      "  [153 133]\n",
      "  [152 133]]\n",
      "\n",
      " [[158 139]\n",
      "  [158 140]\n",
      "  [157 139]\n",
      "  ...\n",
      "  [158 134]\n",
      "  [162 134]\n",
      "  [157 133]]\n",
      "\n",
      " [[157 137]\n",
      "  [159 138]\n",
      "  [158 138]\n",
      "  ...\n",
      "  [151 135]\n",
      "  [127 135]\n",
      "  [120 135]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 37 142]\n",
      "  [ 36 141]\n",
      "  [ 36 140]\n",
      "  ...\n",
      "  [ 93  54]\n",
      "  [105  54]\n",
      "  [123  55]]\n",
      "\n",
      " [[128 141]\n",
      "  [130 141]\n",
      "  [132 141]\n",
      "  ...\n",
      "  [ 73  59]\n",
      "  [121  58]\n",
      "  [125  59]]\n",
      "\n",
      " [[148 140]\n",
      "  [148 140]\n",
      "  [148 139]\n",
      "  ...\n",
      "  [ 63  62]\n",
      "  [131  62]\n",
      "  [124  64]]]\n",
      "[[[159 133]\n",
      "  [ 91 106]\n",
      "  [ 89  81]\n",
      "  ...\n",
      "  [227 164]\n",
      "  [232 187]\n",
      "  [226 212]]\n",
      "\n",
      " [[175 122]\n",
      "  [161 131]\n",
      "  [ 99 131]\n",
      "  ...\n",
      "  [222 176]\n",
      "  [203 168]\n",
      "  [216 156]]\n",
      "\n",
      " [[118 117]\n",
      "  [ 78 144]\n",
      "  [178 128]\n",
      "  ...\n",
      "  [139 191]\n",
      "  [188 184]\n",
      "  [166 175]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[121 187]\n",
      "  [118 188]\n",
      "  [113 178]\n",
      "  ...\n",
      "  [163 238]\n",
      "  [206 232]\n",
      "  [167 231]]\n",
      "\n",
      " [[132 196]\n",
      "  [121 191]\n",
      "  [115 190]\n",
      "  ...\n",
      "  [212 241]\n",
      "  [217 235]\n",
      "  [192 230]]\n",
      "\n",
      " [[127 194]\n",
      "  [111 193]\n",
      "  [123 182]\n",
      "  ...\n",
      "  [223 237]\n",
      "  [211 237]\n",
      "  [163 232]]]\n",
      "[[[102  92]\n",
      "  [ 99  96]\n",
      "  [ 99  84]\n",
      "  ...\n",
      "  [169  94]\n",
      "  [118  93]\n",
      "  [127  80]]\n",
      "\n",
      " [[102  96]\n",
      "  [104  95]\n",
      "  [100  91]\n",
      "  ...\n",
      "  [107  90]\n",
      "  [167  92]\n",
      "  [174  93]]\n",
      "\n",
      " [[ 99  95]\n",
      "  [102  69]\n",
      "  [ 99  82]\n",
      "  ...\n",
      "  [139  61]\n",
      "  [162  84]\n",
      "  [ 79  88]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 37  24]\n",
      "  [ 48  30]\n",
      "  [ 46  33]\n",
      "  ...\n",
      "  [108  28]\n",
      "  [111  27]\n",
      "  [121  27]]\n",
      "\n",
      " [[ 44  27]\n",
      "  [ 48  31]\n",
      "  [ 49  30]\n",
      "  ...\n",
      "  [ 86  27]\n",
      "  [ 94  26]\n",
      "  [ 85  27]]\n",
      "\n",
      " [[ 35  29]\n",
      "  [ 37  29]\n",
      "  [ 43  26]\n",
      "  ...\n",
      "  [ 73  33]\n",
      "  [ 73  32]\n",
      "  [ 80  35]]]\n",
      "[[[ 33  43]\n",
      "  [ 28  42]\n",
      "  [ 29  31]\n",
      "  ...\n",
      "  [ 27  84]\n",
      "  [ 23  50]\n",
      "  [ 26  45]]\n",
      "\n",
      " [[ 32  62]\n",
      "  [ 32  78]\n",
      "  [ 34  40]\n",
      "  ...\n",
      "  [ 27  79]\n",
      "  [ 29  59]\n",
      "  [ 25  75]]\n",
      "\n",
      " [[ 32  31]\n",
      "  [ 30  30]\n",
      "  [ 29  28]\n",
      "  ...\n",
      "  [ 29  70]\n",
      "  [ 24  60]\n",
      "  [ 25  50]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 44  44]\n",
      "  [ 41  41]\n",
      "  [ 42  41]\n",
      "  ...\n",
      "  [153  78]\n",
      "  [157  77]\n",
      "  [ 97  77]]\n",
      "\n",
      " [[ 39  42]\n",
      "  [ 39  42]\n",
      "  [ 40  43]\n",
      "  ...\n",
      "  [135  73]\n",
      "  [156  78]\n",
      "  [107  83]]\n",
      "\n",
      " [[ 39  40]\n",
      "  [ 42  42]\n",
      "  [ 44  42]\n",
      "  ...\n",
      "  [143  89]\n",
      "  [130  95]\n",
      "  [101  94]]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32me:\\Lab\\Homography\\Homogrophy.ipynb Cell 15'\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Lab/Homography/Homogrophy.ipynb#ch0000014?line=42'>43</a>\u001b[0m cv2\u001b[39m.\u001b[39mimshow(\u001b[39m\"\u001b[39m\u001b[39mdestin_image\u001b[39m\u001b[39m\"\u001b[39m,np\u001b[39m.\u001b[39marray(show[:,:,\u001b[39m1\u001b[39m]))\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Lab/Homography/Homogrophy.ipynb#ch0000014?line=43'>44</a>\u001b[0m cv2\u001b[39m.\u001b[39mimshow(\u001b[39m\"\u001b[39m\u001b[39mmodify\u001b[39m\u001b[39m\"\u001b[39m,warped_image)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/Lab/Homography/Homogrophy.ipynb#ch0000014?line=44'>45</a>\u001b[0m \u001b[39mif\u001b[39;00m cv2\u001b[39m.\u001b[39;49mwaitKey(\u001b[39m0\u001b[39;49m) \u001b[39m==\u001b[39m\u001b[39m27\u001b[39m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Lab/Homography/Homogrophy.ipynb#ch0000014?line=45'>46</a>\u001b[0m     cv2\u001b[39m.\u001b[39mdestroyAllWindows()\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Lab/Homography/Homogrophy.ipynb#ch0000014?line=46'>47</a>\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%matplotlib qt5\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import cv2\n",
    "from numpy.linalg import inv\n",
    "\n",
    "test_path = \"./testprocessedfromTrain/\"\n",
    "TestData = Cocodataset(test_path)\n",
    "print(\"the TestData set are\",TestData.len)\n",
    "\n",
    "batch_size = 1\n",
    "TestLoder = DataLoader(TestData,batch_size)\n",
    "model = Model().cuda()\n",
    "model.load_state_dict(torch.load(\"DeepHomographyNet.pth\")[\"state_dict\"])\n",
    "\n",
    "patch_size = 128\n",
    "top_point = (0,0)\n",
    "left_point = (patch_size,0)\n",
    "bottom_point = (patch_size,patch_size)\n",
    "right_point = (0,patch_size)\n",
    "four_points = [top_point,left_point,bottom_point,right_point]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i,(images,target) in enumerate(TestLoder):\n",
    "        show = images[0]\n",
    "        images = images.cuda()\n",
    "        images = images.permute(0,3,1,2).float()\n",
    "        outputs = model(images)\n",
    "        outputs = outputs.view(-1,2)\n",
    "        outputs = outputs.cpu()\n",
    "        outputs = outputs.numpy()\n",
    "        perturbed_four_points = np.add(np.array(four_points).astype(float), np.array(outputs).astype(float)*32.)\n",
    "        H = cv2.getPerspectiveTransform(np.float32(four_points),np.float32(perturbed_four_points))\n",
    "        H_inverse = inv(H)\n",
    "        show = show.cpu()\n",
    "        show = show.numpy()\n",
    "        show = show*127.5+127.5\n",
    "        show = show.astype(\"uint8\")\n",
    "        warped_image = cv2.warpPerspective(show[:,:,1],H,(320,240))\n",
    "        print(show)\n",
    "        cv2.imshow(\"source_image\",np.array(show[:,:,0]))\n",
    "        cv2.imshow(\"destin_image\",np.array(show[:,:,1]))\n",
    "        cv2.imshow(\"modify\",warped_image)\n",
    "        if cv2.waitKey(0) ==27:\n",
    "            cv2.destroyAllWindows()\n",
    "            break\n",
    "# path = \"./testprocessed\"\n",
    "# lst = os.listdir(path)\n",
    "# for i in lst:\n",
    "#     array = np.load(path+\"/%s\"%i,allow_pickle=True)\n",
    "#     X = array[0]\n",
    "#     print(X)\n",
    "    # cv2.imshow(\"destination\",np.array(X[:,:,0]))\n",
    "    # cv2.imshow(\"source\",np.array(X[:,:,1]))\n",
    "    # cv2.waitKey(0)\n",
    "    # cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for test the basic method that is needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### see how the dataset had been settled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(41, 46)\n",
      "(150, 17)\n",
      "(164, 149)\n",
      "(25, 188)\n"
     ]
    }
   ],
   "source": [
    "# %matplotlib qt5\n",
    "# from matplotlib import pyplot as plt\n",
    "# import cv2\n",
    "# import random\n",
    "# import torch\n",
    "# from numpy.linalg import inv\n",
    "# import numpy as np\n",
    "\n",
    "# img = cv2.imread(\"2.jpg\",0)\n",
    "# img = cv2.resize(img,(320,240))\n",
    "\n",
    "# rho          = 32\n",
    "# patch_size   = 128\n",
    "# top_point    = (32,32)\n",
    "# left_point   = (patch_size+32, 32)\n",
    "# bottom_point = (patch_size+32, patch_size+32)\n",
    "# right_point  = (32, patch_size+32)\n",
    "# test_image = img.copy()\n",
    "# four_points = [top_point, left_point, bottom_point, right_point]\n",
    "\n",
    "# perturbed_four_points = []\n",
    "# for point in four_points:\n",
    "#     perturbed_four_points.append((point[0] + random.randint(-rho,rho),point[1]+random.randint(-rho,rho)))\n",
    "#     print((point[0] + random.randint(-rho,rho),point[1]+random.randint(-rho,rho)))\n",
    "#     # move four point to change the map\n",
    "# H = cv2.getPerspectiveTransform(np.float32(four_points),np.float32(perturbed_four_points))\n",
    "# # use the four point to get the matrix\n",
    "# H_inverse = inv(H)\n",
    "# # does it usefu, it may not in that necessary\n",
    "# warped_iamge = cv2.warpPerspective(img,H_inverse,(320,240))\n",
    "# # use matrix to get the image adjusted\n",
    "# Ip1 = test_image[top_point[1]:bottom_point[1],top_point[0]:bottom_point[0]]\n",
    "# # which is the source img\n",
    "# Ip2 = warped_iamge[top_point[1]:bottom_point[1],top_point[0]:bottom_point[0]]\n",
    "# # which is the goal img                                        \n",
    "# training_image = np.dstack((Ip1,Ip2))\n",
    "# # in dimension we stack two matrix map as one\n",
    "# H_four_points = np.subtract(np.array(perturbed_four_points),np.array(four_points))\n",
    "# # each point in matrix just match and minus each point in another matrix\n",
    "# datum = (training_image,H_four_points)\n",
    "# # make them as a real batch of training, [img1,img2],[minus matrix]\n",
    "# cv2.imshow(\"source\",training_image[:,:,0])\n",
    "# cv2.imshow(\"dest\",training_image[:,:,1])\n",
    "# backimg = cv2.warpPerspective(training_image[:,:,1],H,(320,240))\n",
    "# cv2.imshow(\"back\",backimg)\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8f0993ed2895456e110aad98b96776962d3006e8c58c574c2eefef3624adea21"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
